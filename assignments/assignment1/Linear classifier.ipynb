{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1.2 - Linear classifier\n",
    "\n",
    "In this task, we will implement another machine learning model - a linear classifier. The linear classifier selects weights for each class, by which the value of each attribute must be multiplied and then added together.\n",
    "The class for which this sum is greater is the prediction of the model.\n",
    "\n",
    "In this assignment, you:\n",
    "- practice calculating gradients of various multidimensional functions\n",
    "- implement the calculation of gradients through the linear model and the softmax loss function\n",
    "- implement the process of training a linear classifier\n",
    "- select training parameters in practice\n",
    "\n",
    "Just in case, once again the link to the numpy tutorial:\n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As always, the first thing to do is load the data\n",
    "\n",
    "We will use the same SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Add another channel with ones as a bias term\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with gradients!\n",
    "\n",
    "In this course, we will write many functions that calculate gradients analytically.\n",
    "\n",
    "All functions in which we will calculate gradients will be written according to the same scheme.\n",
    "They will receive at the input the point where the value and the gradient of the function need to be calculated, and at the output they will produce a tuple of two values ​​- the actual value of the function at this point (always one number) and the analytical value of the gradient at the same point (the same dimension as the input).\n",
    "``,\n",
    "def f (x):\n",
    "    \"\" \"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function\n",
    "    grad: np array of float, same shape as x\n",
    "    \"\" \"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "``,\n",
    "\n",
    "A necessary tool during the implementation of code that calculates gradients is the function of its validation. This function computes the gradient numerically and compares the result with the analytically computed gradient.\n",
    "\n",
    "We start by implementing the numeric gradient computation in the check_gradient function in gradient_check.py. This function will take as input to the function of the format specified above, use the value `value` to calculate the numerical gradient and compare it with the analytical one - they should converge.\n",
    "\n",
    "Write a part of a function that calculates the gradient using the numerical derivative for each coordinate. To calculate the derivative, use the so-called two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "All functions shown in the next cell must pass the gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Implement check_gradient function in gradient_check.py\n",
    "# All the functions below should pass the gradient check\n",
    "\n",
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start writing our functions that calculate the analytical gradient\n",
    "\n",
    "Now let's implement the softmax function, which receives the estimates for each class as input and converts them into probabilities from 0 to 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "** Important: ** The practical aspect of calculating this function is that it calculates the exponent of potentially very large numbers - this can lead to very large values ​​in the numerator and denominator outside the float range.\n",
    "\n",
    "Fortunately, there is a simple solution to this problem - before calculating the softmax, subtract from all estimates the maximum value among all estimates:\n",
    "``,\n",
    "predictions - = np.max (predictions)\n",
    "``,\n",
    "(more details here - http://cs231n.github.io/linear-classify/#softmax, section `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement softmax and cross-entropy for single sample\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Make sure it works for big numbers too!\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will implement a cross-entropy loss, which we will use as an error function.\n",
    "In general, cross-entropy is defined as follows:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "where x are all classes, p (x) is the true probability of the sample belonging to the class x, and q (x) is the probability of belonging to the class x, as predicted by the model.\n",
    "In our case, the sample belongs to only one class, the index of which is passed to the function. For it, p (x) is equal to 1, and for the rest of the classes - 0.\n",
    "\n",
    "This makes the function easier to implement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have implemented the functions themselves, we can implement the gradient.\n",
    "\n",
    "It turns out that calculating the gradient becomes much easier if you combine these functions into one, which first calculates the probabilities through softmax, and then uses them to calculate the error function through cross-entropy loss.\n",
    "\n",
    "This softmax_with_cross_entropy function will return both the error value and the gradient across the input parameters. We will check the correctness of the implementation with `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement combined function or softmax and cross entropy and produces gradient\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a training method, we will use stochastic gradient descent (SGD), which works with sample batches.\n",
    "\n",
    "Therefore, all our functions will receive not one example, but a batch, that is, the input will not be a vector from `num_classes` evaluations, but a matrix of dimensions` batch_size, num_classes`. The index of the example in the batch will always be the first dimension.\n",
    "\n",
    "The next step is to rewrite our functions to support batches.\n",
    "\n",
    "The final value of the error function must remain a number, and it is equal to the average error value among all examples in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO Extend combined function so it can receive a 2d array with batch of samples\n",
    "np.random.seed(42)\n",
    "# Test batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Test batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Make sure maximum subtraction for numberic stability is done separately for every sample in the batch\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's implement the linear classifier itself!\n",
    "\n",
    "softmax and cross-entropy receive estimates as input, which are given by the linear classifier.\n",
    "\n",
    "He makes it very simple: for each class there is a set of weights by which the pixels of the picture must be multiplied and added. The resulting number is the class estimate that goes to the softmax input.\n",
    "\n",
    "Thus, a linear classifier can be represented as the multiplication of a vector with pixels by a matrix W of size `num_features, num_classes`. This approach can be easily extended to the case of batch vectors with X pixels of size `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, where` * `is matrix multiplication.\n",
    "\n",
    "Implement the `linear_softmax` function for calculating the linear classifier and weighted gradients in the file` linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float)\n",
    "target_index = np.ones(batch_size, dtype=np.int)\n",
    "\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now regularization\n",
    "\n",
    "We will use L2 regularization for the weights as part of the overall error function.\n",
    "\n",
    "Recall that L2 regularization is defined as\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum <sub> ij </sub> W [i, j] <sup> 2 </sup>\n",
    "\n",
    "Implement a function to compute it and compute the appropriate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement l2_regularization function that implements loss for L2 regularization\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workout!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are in order, let's implement the training process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 691.077785\n",
      "Epoch 1, loss: 712.749169\n",
      "Epoch 2, loss: 721.845799\n",
      "Epoch 3, loss: 715.038916\n",
      "Epoch 4, loss: 710.684216\n",
      "Epoch 5, loss: 772.027078\n",
      "Epoch 6, loss: 865.797149\n",
      "Epoch 7, loss: 878.524957\n",
      "Epoch 8, loss: 895.171605\n",
      "Epoch 9, loss: 883.155392\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement LinearSoftmaxClassifier.fit function\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb2a98093a0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's look at the loss history!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.144\n",
      "Epoch 0, loss: 928.421367\n",
      "Epoch 1, loss: 892.918940\n",
      "Epoch 2, loss: 779.946284\n",
      "Epoch 3, loss: 748.959377\n",
      "Epoch 4, loss: 721.792371\n",
      "Epoch 5, loss: 749.993731\n",
      "Epoch 6, loss: 808.052565\n",
      "Epoch 7, loss: 893.858597\n",
      "Epoch 8, loss: 761.667346\n",
      "Epoch 9, loss: 732.110171\n",
      "Epoch 10, loss: 764.896584\n",
      "Epoch 11, loss: 822.292666\n",
      "Epoch 12, loss: 837.657406\n",
      "Epoch 13, loss: 794.137673\n",
      "Epoch 14, loss: 762.657260\n",
      "Epoch 15, loss: 799.105739\n",
      "Epoch 16, loss: 821.750112\n",
      "Epoch 17, loss: 834.937665\n",
      "Epoch 18, loss: 961.965250\n",
      "Epoch 19, loss: 1057.538373\n",
      "Epoch 20, loss: 993.176711\n",
      "Epoch 21, loss: 845.210360\n",
      "Epoch 22, loss: 933.403822\n",
      "Epoch 23, loss: 954.427109\n",
      "Epoch 24, loss: 974.960416\n",
      "Epoch 25, loss: 879.816303\n",
      "Epoch 26, loss: 817.673254\n",
      "Epoch 27, loss: 772.010485\n",
      "Epoch 28, loss: 846.289280\n",
      "Epoch 29, loss: 918.485491\n",
      "Epoch 30, loss: 956.307776\n",
      "Epoch 31, loss: 1048.972222\n",
      "Epoch 32, loss: 830.680057\n",
      "Epoch 33, loss: 847.645747\n",
      "Epoch 34, loss: 898.785442\n",
      "Epoch 35, loss: 908.266541\n",
      "Epoch 36, loss: 1065.815542\n",
      "Epoch 37, loss: 1054.092413\n",
      "Epoch 38, loss: 797.036224\n",
      "Epoch 39, loss: 853.007231\n",
      "Epoch 40, loss: 1056.158814\n",
      "Epoch 41, loss: 781.262694\n",
      "Epoch 42, loss: 822.243782\n",
      "Epoch 43, loss: 851.709586\n",
      "Epoch 44, loss: 904.160039\n",
      "Epoch 45, loss: 788.509429\n",
      "Epoch 46, loss: 904.579533\n",
      "Epoch 47, loss: 897.834099\n",
      "Epoch 48, loss: 840.650563\n",
      "Epoch 49, loss: 845.823594\n",
      "Epoch 50, loss: 821.645579\n",
      "Epoch 51, loss: 951.463899\n",
      "Epoch 52, loss: 952.506860\n",
      "Epoch 53, loss: 890.513110\n",
      "Epoch 54, loss: 835.853571\n",
      "Epoch 55, loss: 865.995029\n",
      "Epoch 56, loss: 953.318479\n",
      "Epoch 57, loss: 728.511926\n",
      "Epoch 58, loss: 758.240747\n",
      "Epoch 59, loss: 819.868020\n",
      "Epoch 60, loss: 909.254780\n",
      "Epoch 61, loss: 913.225182\n",
      "Epoch 62, loss: 854.503942\n",
      "Epoch 63, loss: 854.296190\n",
      "Epoch 64, loss: 890.855868\n",
      "Epoch 65, loss: 1014.158191\n",
      "Epoch 66, loss: 1106.199824\n",
      "Epoch 67, loss: 957.268153\n",
      "Epoch 68, loss: 916.015081\n",
      "Epoch 69, loss: 936.095907\n",
      "Epoch 70, loss: 896.116819\n",
      "Epoch 71, loss: 831.130963\n",
      "Epoch 72, loss: 845.144261\n",
      "Epoch 73, loss: 862.572171\n",
      "Epoch 74, loss: 924.765478\n",
      "Epoch 75, loss: 944.146242\n",
      "Epoch 76, loss: 901.503518\n",
      "Epoch 77, loss: 1017.109238\n",
      "Epoch 78, loss: 955.668836\n",
      "Epoch 79, loss: 913.752116\n",
      "Epoch 80, loss: 970.836394\n",
      "Epoch 81, loss: 919.695255\n",
      "Epoch 82, loss: 893.641933\n",
      "Epoch 83, loss: 929.417188\n",
      "Epoch 84, loss: 787.949758\n",
      "Epoch 85, loss: 813.064688\n",
      "Epoch 86, loss: 720.630810\n",
      "Epoch 87, loss: 728.198767\n",
      "Epoch 88, loss: 726.012804\n",
      "Epoch 89, loss: 722.066000\n",
      "Epoch 90, loss: 809.261039\n",
      "Epoch 91, loss: 965.052863\n",
      "Epoch 92, loss: 948.555864\n",
      "Epoch 93, loss: 841.439220\n",
      "Epoch 94, loss: 1010.892037\n",
      "Epoch 95, loss: 912.154615\n",
      "Epoch 96, loss: 826.069448\n",
      "Epoch 97, loss: 880.076778\n",
      "Epoch 98, loss: 894.175813\n",
      "Epoch 99, loss: 921.670196\n",
      "Accuracy after training for 100 epochs:  0.166\n"
     ]
    }
   ],
   "source": [
    "# Let's check how it performs on validation set\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Now, let's train more and see if it performs better\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-3, batch_size=300, reg=1e1, verbose=1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As before, we use cross-validation to find hyperparameters.\n",
    "\n",
    "This time, we will only use one division of training and validation data to keep the training time reasonable.\n",
    "\n",
    "Now we need to select not one, but two hyperparameters! Don't limit yourself to the original values in the code.\n",
    "Achieve more than ** 20% ** accuracy on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best validation accuracy achieved: 0.227000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# TODO use validation set to find the best hyperparameters\n",
    "# hint: for best results, you might need to try more values for learning rate and regularization strength \n",
    "# than provided initially\n",
    "\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "        classifier.fit(train_X, train_y, epochs=num_epochs,\n",
    "                       learning_rate=learning_rate, batch_size=batch_size, reg=reg)\n",
    "        pred = classifier.predict(val_X)\n",
    "        accuracy = multiclass_accuracy(pred, val_y)\n",
    "        if accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = accuracy\n",
    "            best_classifier = classifier        \n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What accuracy did we achieve on the test data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.176000\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
